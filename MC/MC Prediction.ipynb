{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "\n",
    "    # Implement this!\n",
    "    for i_episode in range(1, 1 + num_episodes):\n",
    "        if i_episode % 10000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # Following the policy to play the game and record states\n",
    "        episode_states = []\n",
    "        observation = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(observation)\n",
    "            new_observation, reward, is_done, _ = env.step(action)\n",
    "            episode_states.append((observation, reward, is_done))\n",
    "            observation = new_observation\n",
    "\n",
    "        # Evaluating policy (updating value function)\n",
    "        for state in set(tuple(x[0]) for x in episode_states):\n",
    "            first_visit_of_state = next(i for i, x in enumerate(episode_states) if x[0] == state)\n",
    "            g = sum(x[1] * discount_factor ** i for i, x in enumerate(episode_states[first_visit_of_state:]))\n",
    "            returns_count[state] += 1\n",
    "            returns_sum[state] += g\n",
    "\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_every_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "\n",
    "    # Implement this!\n",
    "    for i_episode in range(1, 1 + num_episodes):\n",
    "        if i_episode % 10000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # Following the policy to play the game and record states\n",
    "        episode_states = []\n",
    "        observation = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(observation)\n",
    "            new_observation, reward, is_done, _ = env.step(action)\n",
    "            episode_states.append((observation, reward, is_done))\n",
    "            observation = new_observation\n",
    "\n",
    "        # Evaluating policy (updating value function)\n",
    "        for state in set(tuple(x[0]) for x in episode_states):\n",
    "            \n",
    "            visits_of_state = set(i for i, x in enumerate(episode_states) if x[0] == state)\n",
    "            for visit in visits_of_state:\n",
    "                g = sum(x[1] * discount_factor ** i for i, x in enumerate(episode_states[visit:]))\n",
    "                returns_count[state] += 1\n",
    "                returns_sum[state] += g\n",
    "    \n",
    "                V[state] = returns_sum[state] / returns_count[state]\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is > 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 17 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# V_10k = mc_prediction(sample_policy, env, num_episodes=10000)\n",
    "# plotting.plot_value_function(V_10k, title=\"Sample_p 10,000 Steps\")\n",
    "\n",
    "V_500k = mc_prediction(sample_policy, env, num_episodes=500000)\n",
    "plotting.plot_value_function(V_500k, title=\"Sample_p 500,000 Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is > 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    if score < 18 or (usable_ace and score < 20):\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "V_10k = mc_prediction(my_policy, env, num_episodes=10000)\n",
    "plotting.plot_value_function(V_10k, title=\"mc_prediction 10,000 Steps\")\n",
    "\n",
    "# V_500k = mc_prediction(my_policy, env, num_episodes=500000)\n",
    "# plotting.plot_value_function(V_500k, title=\"My_p18  500,000 Steps\")\n",
    "\n",
    "V_10k = mc_every_prediction(my_policy, env, num_episodes=10000)\n",
    "plotting.plot_value_function(V_10k, title=\"mc_every_prediction 10,000 Steps\")\n",
    "\n",
    "V_10k = td_prediction(my_policy, env, num_episodes=10000)\n",
    "plotting.plot_value_function(V_10k, title=\"td_prediction 10,000 Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def winning_prob(policy, num_episodes):\n",
    "    total_reward = 0\n",
    "    player = 0\n",
    "    dealer = 0\n",
    "    flat = 0\n",
    "    black_jack = 0\n",
    "    # Implement this!\n",
    "    for i_episode in range(1, 1 + num_episodes):\n",
    "        if i_episode % 10000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # Following the policy to play the game and record states\n",
    "        observation = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(observation)\n",
    "            new_observation, reward, is_done, _ = env.step(action)\n",
    "            observation = new_observation\n",
    "        reward_i = env.get_player_reward()\n",
    "        total_reward += reward_i\n",
    "        # print(str(env.dealer) + ':' + str(env.player) + ' :: ' + str(reward_i))\n",
    "        if reward_i == 0:\n",
    "            flat += 1\n",
    "        elif reward_i > 0:\n",
    "            player += 1\n",
    "            if reward_i == 1.5:\n",
    "                black_jack += 1\n",
    "        else:\n",
    "            dealer += 1\n",
    "    return total_reward, player / num_episodes, flat / num_episodes, dealer / num_episodes, black_jack\n",
    "\n",
    "print(\"total_reward, player/num_episodes, flat/num_episodes, dealer/num_episodes, black_jack_count\")\n",
    "print('The percentage of wining with my_policy is ' + str(winning_prob(my_policy, 10000)))\n",
    "print('The percentage of wining with policy hit smaller than 17 is ' + str(winning_prob(sample_policy, 10000)))\n",
    "print('The percentage of wining with RL policy is ' + str(winning_prob(policy, 10000)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
